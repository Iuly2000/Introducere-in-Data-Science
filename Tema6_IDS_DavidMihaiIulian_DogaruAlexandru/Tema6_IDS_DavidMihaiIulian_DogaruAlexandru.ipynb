{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.20.1\n",
      "Pandas version: 1.2.3\n",
      "Sklearn version: 0.24.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "\n",
    "import pandas as pd\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from collections import Counter\n",
    "print(f'Sklearn version: {sk.__version__}')\n",
    "\n",
    "#NumPy version: 1.20.1\n",
    "#Pandas version: 1.2.3\n",
    "#Sklearn version: 0.24.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laborator 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folosind un set de date - de exemplu de la https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=taskDown&view=table - sa se rezolve o problema de clasificare sau regresie, plecand de la intrari de tip text.\n",
    "\n",
    "Se poate opta pentru codificare BOW, n-grams, word2vec sau altele adecvate. Modelele de predictie pot fi din biblioteca scikit-learn. Puteti folosi pentru preprocesare biblioteca [NLTK](https://www.nltk.org).\n",
    "\n",
    "Pentru clasificare se va optimiza scorul F1; se vor raporta scorurile F1 si acuratetea. Pentru regresie se va optimiza scorul mean squared error; se vor raporta scorurile MSE, mean absolute error, r2.\n",
    "\n",
    "Exemple:\n",
    "1. [Clasificare de SMS-uri](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "1. [Sentence Classification Data Set](https://archive.ics.uci.edu/ml/datasets/Sentence+Classification#)\n",
    "1. [Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)\n",
    "1. [Victorian Era Authorship Attribution Data Set](https://archive.ics.uci.edu/ml/datasets/Victorian+Era+Authorship+Attribution)\n",
    "1. [Amazon Commerce reviews set Data Set](https://archive.ics.uci.edu/ml/datasets/Amazon+Commerce+reviews+set)\n",
    "1. [Farm Ads Data Set](https://archive.ics.uci.edu/ml/datasets/Farm+Ads)\n",
    "\n",
    "\n",
    "Se vor investiga minim 2 seturi de date si pentru fiecare din ele minim 4 modele de clasificare sau de regresie. Daca setul de date e deja impartit in train si test, se va folosi ca atare - setul de antrenare se va imparti, suplimentar in train + validation; altfel, se va face  5 fold CV. Valorile optime ale hiperparametrilor vor fi alese prin random search sau grid search.\n",
    "\n",
    "Pentru fiecare set de date:\n",
    "1. (2 x 0.5 p) Se descrie setul de date, in limba romana (continut, provenienta, problema etc.)\n",
    "1. (2 x 1 p) Se face analiza exploratorie, folosind cod Python: distributia claselor sau a valorilor continue de iesire - numeric si grafic, statistici asupra textelor (de exemplu: lungime minima/medie/maxima; cele mai frecvente k cuvinte; clustering etc.). Se va explica fiecare pas si ce se urmareste prin efectuarea lui. Graficele vor avea axele numite (ce se reprezinta, evetual unitate de masura)\n",
    "1. (2 x 0.5 p) Se face preprocesare de date; se explica in limba romana care sunt metodele de preprocesare folosite, efectul lor pe datele de intrare, ce forma are iesirea obtinuta; se arata efectele pasilor de preprocesare asupra setului de date (noul numar de documente, dinamica vocabularului, trasaturile rezultate etc.) Se pot aduga grafice si tabele la acest pas.\n",
    "1. (2 x 4 x 0.5 p) Clasificare sau regresie, dupa caz: se face o descriere a modelelor considerate, in limba romana; se descrie modalitatea de cautare a hiperparametrilor; rezultatele obtinute se vor prezenta tabelar, similar cu tema precedenta. \n",
    "\n",
    "Se acorda doua puncte din oficiu.\n",
    "\n",
    "Descrierea modelelor si a pasilor de preprocesare pot fi in sectiuni separate, cu referinte la acestea unde e necesar.\n",
    "\n",
    "Exemple:\n",
    "1. [Working With Text Data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "1. [Text Classification with Python and Scikit-Learn](https://stackabuse.com/text-classification-with-python-and-scikit-learn/)\n",
    "1. [How to Prepare Text Data for Machine Learning with scikit-learn](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics_regression(reg, parameters: dict, x: np.ndarray, y: np.ndarray) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Shows the metrics('mean_absolute_error', 'mean_squared_error', 'median_absolute_error') of a regressor.\n",
    "    \n",
    "    Args:\n",
    "        reg: a regressor\n",
    "        parameters:a dictionary containning the hiperparameters\n",
    "        x: np.array containning the dataset information\n",
    "        y: np.array containning the classification of the data\n",
    "        \n",
    "    Returns:\n",
    "        a pandas dataframe with the metrics of a regressor\n",
    "    \"\"\"\n",
    "    gridsrc = GridSearchCV(estimator=reg, \n",
    "            param_grid=parameters, cv=3, n_jobs=-1, return_train_score=True)   \n",
    "    scoring = ['neg_mean_absolute_error', 'neg_mean_squared_error']\n",
    "    scores1 = cross_validate(gridsrc, x, y, cv=5, scoring=scoring, return_train_score=True)    \n",
    "    \n",
    "    df1 = pd.DataFrame(data={'train_neg_mean_absolute_error': scores1['train_neg_mean_absolute_error'],\n",
    "                            'train_neg_mean_squared_error': scores1['train_neg_mean_squared_error'],                            \n",
    "                            'test_neg_mean_absolute_error': scores1['test_neg_mean_absolute_error'],\n",
    "                            'test_neg_mean_squared_error': scores1['test_neg_mean_squared_error']                            \n",
    "                           })    \n",
    "    \n",
    "    result = pd.DataFrame([df1.mean()])\n",
    "    result.insert(0, 'Model_name', [reg])\n",
    "    result.insert(1, 'Search_strategy', ['GridSearchCV'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_SGDRegressor:dict = {\n",
    "    'max_iter':[10000],\n",
    "    'loss': ['squared_loss','epsilon_insensitive','squared_epsilon_insensitive'],\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'alpha' : [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "    \n",
    "parameters_RandomForestRegressor:dict = {\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [10, 50, 100]    \n",
    "}\n",
    "    \n",
    "parameters_Lasso:dict = {\n",
    "    'alpha':[0.01,0.1,1],\n",
    "    'tol':[0.0001,0.001,0.01,0.1],\n",
    "    'selection':['cyclic','random']\n",
    "}\n",
    "    \n",
    "parameters_MLPRegressor:dict = {\n",
    "    'max_iter':[10000],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.001, 0.01, 0.1, 1],\n",
    "    'tol':[0.0001, 0.001, 0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_regression(name:str, x:np.ndarray, y:np.ndarray) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    This function calls the show_metrics_regression for every regression\n",
    "    and concatenates the resulting dataframes.\n",
    "    \n",
    "    Args:\n",
    "        name:the name of the dataset\n",
    "        x:the content of the dataset\n",
    "        y:the target of the dataset\n",
    "        \n",
    "    Returns:\n",
    "        a dataframe                \n",
    "    \"\"\"\n",
    "    df1 = show_metrics_regression(SGDRegressor(), parameters_SGDRegressor, x, y)\n",
    "    df2 = show_metrics_regression(RandomForestRegressor(),parameters_RandomForestRegressor, x, y)\n",
    "    df3 = show_metrics_regression(Lasso(), parameters_Lasso, x, y)\n",
    "    df4 = show_metrics_regression(MLPRegressor(), parameters_MLPRegressor, x, y)    \n",
    "\n",
    "    df = pd.concat([df1, df2, df3, df4], axis=0, ignore_index=True)\n",
    "    df.columns.name = name\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s) -> list:    \n",
    "    \"\"\"\n",
    "    Highlight the maximum in a dataframe red for maximum and green for minimum.\n",
    "    \n",
    "    Returns:\n",
    "        a list of strings representing the color\n",
    "    \"\"\"\n",
    "    \n",
    "    max_val:float = s.max()\n",
    "    min_val:float = s.min()\n",
    "    return ['background-color: #ff6666' if v==max_val and type(v)\n",
    "            else 'background-color: #bdfcc2'if v==min_val else '' for v in s]\n",
    "\n",
    "def finishing(df:pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    This functions transforms the metrics of the dataframe from negative to positive\n",
    "    and highlights the column with the min and max from dataframe\n",
    "    \n",
    "    Args:\n",
    "        df:a dataframe\n",
    "    \n",
    "    Returns:\n",
    "        a dataframe with positive numbers and highlights the maximum and minimum.\n",
    "    \"\"\"\n",
    "    aux = df.columns.name\n",
    "    df.iloc[:, 2:] = df.iloc[:, 2:].abs()\n",
    "\n",
    "    df.columns = ['Model_name', 'Search_strategy',\n",
    "                  'train_mean_absolute_error', 'train_mean_squared_error',\n",
    "                   'test_mean_absolute_error','test_mean_squared_error']\n",
    "\n",
    "    df = df.style.apply(highlight_max, subset=df.columns[2:])\n",
    "    df.columns.name = aux\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_dataframe(data:np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    This function calculates the the minimum and maximum lenght of a text.\n",
    "    Also the mean lenght of the texts.\n",
    "    \n",
    "    Parameters:\n",
    "    df:\n",
    "        a np.ndarray\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    texts_lenght:list[int] = [len(text) for text in data]\n",
    "    print(f'Minimum lenght:{np.min(texts_lenght)}')\n",
    "    print(f'Maximum lenght:{np.max(texts_lenght)}')\n",
    "    print(f'Mean lenght:{np.mean(texts_lenght)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_words(data:np.ndarray,k=5) -> None:\n",
    "    \"\"\"\n",
    "    This function calculates the k most frequent words in a text.\n",
    "    \n",
    "    Args:\n",
    "        data:the text\n",
    "        k:the number of words\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    words:list = [word for text in data for word in text.split(sep=' ') if len(word)>2]  \n",
    "    words_to_count:generator = (word for word in words)    \n",
    "    c:collections.Counter = Counter(words_to_count)\n",
    "    print(f'Most frequent {k} words:{c.most_common(k)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Information: SMSSpamCollection\n",
    "\n",
    "Acest dataset a fost colectat de la surse de cercetare libere de pe internet:\n",
    "\n",
    "-> O colectie de 425 SMS mesaje spam extrase manual de pe  site-ul Grumbletext. Acesta este un forum din UK in care utilizatorii de smartphones revendica mesajele spam, majoritatea fara sa reporteze mesajul receptionat. Identificarea  mesajelor spam revendicate este un lucru foarte indelungat, implicand scanarea a sute de pagini web. Site-ul Grumbletext este: [Web Link](http://www.grumbletext.co.uk/).\n",
    "-> Un subset de 3,375 de sms-uri ham alese la intamplare din NUS SMS Corpus (NSC), care este un dataset de 10,0000 mesaje legitime, colectate pentru cercetare la Departamentul de Computer Science de la universitatea nationala din Singapore. Majoritatea mesajelor provin de la singaporezi si studenti din universitate. Mesajele au fost colectate de la voluntari carora li s-au spus ca aceste contributii vor fi facute publice.\n",
    "-> O lista de 450 de SMS ham colectate din teza Caroline Tag's PhD disponibila la: [Web Link](https://etheses.bham.ac.uk/id/eprint/253/1/Tagg09PhD.pdf).\n",
    "-> In final, a fost incorporat SMS Spam Corpus v.0.1 Big. Are 1,002 mesaje ham si 322 mesaje spam. Acest dataset a fost folosit in urmatoarele cercetari academice:\n",
    "\n",
    "\n",
    "[1] GÃ³mez Hidalgo, J.M., Cajigas Bringas, G., Puertas Sanz, E., Carrero GarcÃ­a, F. Continut bazat pe filtrarea spamurilor.2006 ACM Simpozion pe documentul de inginerie (ACM DOCENG'06), Amsterdam, The Netherlands, 10-13, 2006.\n",
    "\n",
    "[2] Cormack, G. V., GÃ³mez Hidalgo, J. M., and Puertas SÃ¡nz, E. Feature pentru infineria de telefon mobil (SMS) spam filtrare.A 30-a conferinta internationala anuala ACM de cercetare si dezvoltare in recuperare de informatie(ACM SIGIR'07), New York, NY, 871-872, 2007.\n",
    "\n",
    "[3] Cormack, G. V., GÃ³mez Hidalgo, J. M., and Puertas SÃ¡nz, E. Filtrare spam pentru mesaje scurte. A 16-a conferinta ACM  de administrare de informatie si cunostinte(ACM CIKM'07). Lisbon, Portugal, 313-320, 2007."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "smsdata:pd.core.frame.DataFrame = pd.read_csv('data/SMSSpamCollection', sep = '\\t', names=[\"label\", \"sms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   5572 non-null   object\n",
      " 1   sms     5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "smsdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                                sms\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_x_sms:np.ndarray = smsdata[\"sms\"].values\n",
    "text_y_sms:np.ndarray = smsdata[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are the texts organized:  <class 'numpy.ndarray'>\n",
      "How many texts: 5572\n",
      "A text:  Need a coffee run tomo?Can't believe it's that time of week already\n",
      "Associated label: ham\n"
     ]
    }
   ],
   "source": [
    "print('How are the texts organized: ', type(text_x_sms))\n",
    "print('How many texts:', len(text_x_sms))\n",
    "print('A text: ', text_x_sms[300])\n",
    "print('Associated label:', text_y_sms[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum lenght:2\n",
      "Maximum lenght:910\n",
      "Mean lenght:80.48994974874371\n"
     ]
    }
   ],
   "source": [
    "metrics_dataframe(text_x_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent 4 words:[('you', 1626), ('the', 1207), ('and', 858), ('for', 650)]\n"
     ]
    }
   ],
   "source": [
    "frequent_words(text_x_sms,k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificare BOW\n",
    "\n",
    "Una dintre cele mai mari probleme cu un text este faptul ca este foarte dezordonat si nestructurat, iar algoritmi de machine learning prefera totul structurat si input-uri bine definite. Prin intermediul tehnici Bag-of-Words putem converti variabilele lungi de text in vectori cu lungime fixa. De asemenea, modelel de machine learning lucreaza cu date numerice mai mult decat date textuale. Mai specific, folosind tehnica bag-of-words (BoW), convertim un text in vectorul de numere echivalent acestuia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "Folosim functia CountVectorizer din libraria Sk-learn pentru a implementa usor modelul BoW precizat anterior.\n",
    "\n",
    "CountVectorizer produce o cale simpla atat pentru a tokeniza o colectie de documente text cat si a construi un vocabular de cuvinte cunoscute, de asemenea codifica si noi documente folosind acel vocabular.\n",
    "\n",
    "\n",
    "Primul parametru este max_features, care este setat la 1500, deoarece cand convertesti cuvinte la numere folosing BOW, toate cuvintele unice din toate documentele sunt convertite in feature-uri. Toate documentele pot contine zeci de mii de cuvinte unice, dar cuvintele care au o frecventa joasa de aparitie nu sunt un parametru bun de clasificare a documentelor. Prin urmare setam max_features la 1500, ceea ce inseamna ca vrem sa folosim 1500 cele mai frecvente cuvinte ca si feature-uri pentru antrenarea clasificatorului.\n",
    "\n",
    "\n",
    "Al doilea parametru este min_df care este setat la 5. Asta corespunde cu numarul minim de documente care ar trebui sa contina aces feature. Deci includem numai acele cuvinte care apar in cel putin 5 documente. Similar pentru max_df, valoare este setata la 0.7, ceea ce corespunde unui procentaj. Aici 0.7 inseamna ca ar trebui sa includa numai acele cuvinte care apar in maximum 70% din toate documentele.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect:sk.feature_extraction.text.CountVectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In continuare este returnat un vector codat cu lungimea intregului vocabular si un contor pentru fiecare aparitie a unui cuvant in document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 537)\t1\n",
      "  (0, 1350)\t1\n",
      "  (0, 1004)\t1\n",
      "  (0, 331)\t1\n",
      "  (0, 151)\t1\n",
      "  (0, 931)\t1\n",
      "  (0, 647)\t1\n",
      "  (0, 224)\t1\n",
      "  (0, 551)\t1\n",
      "  (0, 1462)\t1\n",
      "  (0, 702)\t1\n",
      "  (0, 279)\t1\n",
      "  (0, 1263)\t1\n",
      "  (0, 547)\t1\n",
      "  (0, 1400)\t1\n",
      "  (1, 920)\t1\n",
      "  (1, 707)\t1\n",
      "  (1, 678)\t1\n",
      "  (1, 1434)\t1\n",
      "  (2, 647)\t1\n",
      "  (2, 496)\t1\n",
      "  (2, 436)\t2\n",
      "  (2, 1450)\t1\n",
      "  (2, 302)\t1\n",
      "  (2, 1292)\t3\n",
      "  :\t:\n",
      "  (5570, 496)\t1\n",
      "  (5570, 1292)\t1\n",
      "  (5570, 581)\t1\n",
      "  (5570, 666)\t1\n",
      "  (5570, 1410)\t1\n",
      "  (5570, 113)\t1\n",
      "  (5570, 737)\t1\n",
      "  (5570, 1170)\t1\n",
      "  (5570, 488)\t1\n",
      "  (5570, 1258)\t1\n",
      "  (5570, 176)\t1\n",
      "  (5570, 885)\t1\n",
      "  (5570, 370)\t1\n",
      "  (5570, 1357)\t1\n",
      "  (5570, 227)\t1\n",
      "  (5570, 1174)\t1\n",
      "  (5570, 423)\t1\n",
      "  (5570, 520)\t1\n",
      "  (5570, 229)\t1\n",
      "  (5570, 557)\t1\n",
      "  (5570, 655)\t1\n",
      "  (5571, 1292)\t1\n",
      "  (5571, 873)\t1\n",
      "  (5571, 667)\t2\n",
      "  (5571, 1321)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vect.fit_transform(text_x_sms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sms:np.ndarray = vect.fit_transform(text_x_sms).toarray()\n",
    "y_sms:np.ndarray = le.fit_transform(text_y_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiunea vocabularului: 1500\n"
     ]
    }
   ],
   "source": [
    "feature_names:list = vect.get_feature_names()\n",
    "print('Dimensiunea vocabularului:', len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '02', '03', '04', '06', '0800', '08000839402', '08000930705', '0870', '08712460324', '08718720201', '10', '100', '1000', '10am', '10p', '11', '11mths', '12']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yahoo', 'yar', 'yeah', 'year', 'years', 'yep', 'yes', 'yest', 'yesterday', 'yet', 'yijue', 'yo', 'yoga', 'you', 'your', 'yours', 'yourself', 'yr', 'yrs', 'yup']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_daad8_row1_col2,#T_daad8_row1_col3,#T_daad8_row1_col4,#T_daad8_row3_col5{\n",
       "            background-color:  #bdfcc2;\n",
       "        }#T_daad8_row2_col2,#T_daad8_row2_col3,#T_daad8_row2_col4,#T_daad8_row2_col5{\n",
       "            background-color:  #ff6666;\n",
       "        }</style><table id=\"T_daad8_\" ><thead>    <tr>        <th class=\"index_name level0\" >SMSSpamCollection_Dataset</th>        <th class=\"col_heading level0 col0\" >Model_name</th>        <th class=\"col_heading level0 col1\" >Search_strategy</th>        <th class=\"col_heading level0 col2\" >train_mean_absolute_error</th>        <th class=\"col_heading level0 col3\" >train_mean_squared_error</th>        <th class=\"col_heading level0 col4\" >test_mean_absolute_error</th>        <th class=\"col_heading level0 col5\" >test_mean_squared_error</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_daad8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_daad8_row0_col0\" class=\"data row0 col0\" >SGDRegressor()</td>\n",
       "                        <td id=\"T_daad8_row0_col1\" class=\"data row0 col1\" >GridSearchCV</td>\n",
       "                        <td id=\"T_daad8_row0_col2\" class=\"data row0 col2\" >0.071283</td>\n",
       "                        <td id=\"T_daad8_row0_col3\" class=\"data row0 col3\" >0.017270</td>\n",
       "                        <td id=\"T_daad8_row0_col4\" class=\"data row0 col4\" >0.082515</td>\n",
       "                        <td id=\"T_daad8_row0_col5\" class=\"data row0 col5\" >0.022951</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_daad8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_daad8_row1_col0\" class=\"data row1 col0\" >RandomForestRegressor()</td>\n",
       "                        <td id=\"T_daad8_row1_col1\" class=\"data row1 col1\" >GridSearchCV</td>\n",
       "                        <td id=\"T_daad8_row1_col2\" class=\"data row1 col2\" >0.023676</td>\n",
       "                        <td id=\"T_daad8_row1_col3\" class=\"data row1 col3\" >0.007019</td>\n",
       "                        <td id=\"T_daad8_row1_col4\" class=\"data row1 col4\" >0.045939</td>\n",
       "                        <td id=\"T_daad8_row1_col5\" class=\"data row1 col5\" >0.023677</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_daad8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_daad8_row2_col0\" class=\"data row2 col0\" >Lasso()</td>\n",
       "                        <td id=\"T_daad8_row2_col1\" class=\"data row2 col1\" >GridSearchCV</td>\n",
       "                        <td id=\"T_daad8_row2_col2\" class=\"data row2 col2\" >0.152425</td>\n",
       "                        <td id=\"T_daad8_row2_col3\" class=\"data row2 col3\" >0.063174</td>\n",
       "                        <td id=\"T_daad8_row2_col4\" class=\"data row2 col4\" >0.153493</td>\n",
       "                        <td id=\"T_daad8_row2_col5\" class=\"data row2 col5\" >0.064383</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_daad8_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_daad8_row3_col0\" class=\"data row3 col0\" >MLPRegressor()</td>\n",
       "                        <td id=\"T_daad8_row3_col1\" class=\"data row3 col1\" >GridSearchCV</td>\n",
       "                        <td id=\"T_daad8_row3_col2\" class=\"data row3 col2\" >0.051834</td>\n",
       "                        <td id=\"T_daad8_row3_col3\" class=\"data row3 col3\" >0.012641</td>\n",
       "                        <td id=\"T_daad8_row3_col4\" class=\"data row3 col4\" >0.063719</td>\n",
       "                        <td id=\"T_daad8_row3_col5\" class=\"data row3 col5\" >0.019776</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d7bddb9910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_smsdata:pd.core.frame.DataFrame = dataset_regression('SMSSpamCollection_Dataset',x_sms,y_sms)\n",
    "display(finishing(df_smsdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/farm-ads\", \"r\")\n",
    "lines:list = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_ads_data:pd.core.frame.DataFrame = pd.DataFrame([line.split(' ', 1) for line in lines], columns=['label', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4143 entries, 0 to 4142\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   4143 non-null   object\n",
      " 1   text    4143 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 64.9+ KB\n"
     ]
    }
   ],
   "source": [
    "farm_ads_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-jerry ad-bruckheimer ad-chase ad-premier ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-rheumatoid ad-arthritis ad-expert ad-tip ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-rheumatologist ad-anju ad-varghese ad-yonke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-siemen ad-water ad-remediation ad-water ad-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-symptom ad-muscle ad-weakness ad-genetic ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-affordable ad-ivf ad-cost ad-efficient ad-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-mozypro ad-business ad-backup ad-affordable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4140</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-oster ad-line ad-clipper ad-oster ad-factor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-synrevoice ad-schoolconnect ad-trust ad-aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4142</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-vet ad-online ad-veterinarian ad-online ad-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4143 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0        1  ad-jerry ad-bruckheimer ad-chase ad-premier ad...\n",
       "1       -1  ad-rheumatoid ad-arthritis ad-expert ad-tip ad...\n",
       "2       -1  ad-rheumatologist ad-anju ad-varghese ad-yonke...\n",
       "3       -1  ad-siemen ad-water ad-remediation ad-water ad-...\n",
       "4       -1  ad-symptom ad-muscle ad-weakness ad-genetic ad...\n",
       "...    ...                                                ...\n",
       "4138    -1  ad-affordable ad-ivf ad-cost ad-efficient ad-i...\n",
       "4139     1  ad-mozypro ad-business ad-backup ad-affordable...\n",
       "4140     1  ad-oster ad-line ad-clipper ad-oster ad-factor...\n",
       "4141    -1  ad-synrevoice ad-schoolconnect ad-trust ad-aut...\n",
       "4142     1  ad-vet ad-online ad-veterinarian ad-online ad-...\n",
       "\n",
       "[4143 rows x 2 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farm_ads_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_x_farm_ads:np.ndarray = farm_ads_data['text'].values\n",
    "text_y_farm_ads:np.ndarray = farm_ads_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are the texts organized:  <class 'numpy.ndarray'>\n",
      "How many texts: 4143\n",
      "A text:  ad-embryo ad-storage ad-experience ad-exceptional ad-service ad-low ad-rate ad-www ad-fairfaxcryobank ad-com title-found header-found found found request url emstorage aspx found server additionally found error encounter try errordocument handle request apache cento mod ssl dav mod auth passthrough frontpage server www fairfaxcryobank com port\n",
      "\n",
      "Associated label: -1\n"
     ]
    }
   ],
   "source": [
    "print('How are the texts organized: ', type(text_x_farm_ads))\n",
    "print('How many texts:', len(text_x_farm_ads))\n",
    "print('A text: ', text_x_farm_ads[256])\n",
    "print('Associated label:', text_y_farm_ads[256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum lenght:32\n",
      "Maximum lenght:63196\n",
      "Mean lenght:3220.6830798937967\n"
     ]
    }
   ],
   "source": [
    "metrics_dataframe(text_x_farm_ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent 4 words:[('list', 21454), ('product', 17598), ('com', 11628), ('health', 8432)]\n"
     ]
    }
   ],
   "source": [
    "frequent_words(text_x_farm_ads,k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect:sk.feature_extraction.text.CountVectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7)\n",
    "x_farm_ads:np.ndarray = vect.fit_transform(text_x_farm_ads).toarray()\n",
    "y_farm_ads:np.ndarray = le.fit_transform(text_y_farm_ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1356)\t1\n",
      "  (0, 251)\t1\n",
      "  (0, 957)\t1\n",
      "  (0, 549)\t1\n",
      "  (1, 1158)\t21\n",
      "  (1, 78)\t22\n",
      "  (1, 479)\t2\n",
      "  (1, 1365)\t5\n",
      "  (1, 689)\t1\n",
      "  (1, 79)\t1\n",
      "  (1, 1391)\t5\n",
      "  (1, 938)\t3\n",
      "  (1, 1324)\t1\n",
      "  (1, 1410)\t5\n",
      "  (1, 470)\t7\n",
      "  (1, 623)\t22\n",
      "  (1, 621)\t3\n",
      "  (1, 1163)\t5\n",
      "  (1, 15)\t3\n",
      "  (1, 290)\t3\n",
      "  (1, 293)\t1\n",
      "  (1, 1251)\t1\n",
      "  (1, 568)\t2\n",
      "  (1, 1089)\t13\n",
      "  (1, 31)\t1\n",
      "  :\t:\n",
      "  (4142, 1430)\t1\n",
      "  (4142, 614)\t1\n",
      "  (4142, 1197)\t1\n",
      "  (4142, 926)\t1\n",
      "  (4142, 1100)\t1\n",
      "  (4142, 757)\t1\n",
      "  (4142, 212)\t1\n",
      "  (4142, 1164)\t1\n",
      "  (4142, 977)\t2\n",
      "  (4142, 898)\t1\n",
      "  (4142, 413)\t1\n",
      "  (4142, 729)\t1\n",
      "  (4142, 1177)\t1\n",
      "  (4142, 531)\t1\n",
      "  (4142, 438)\t1\n",
      "  (4142, 713)\t1\n",
      "  (4142, 1065)\t1\n",
      "  (4142, 714)\t1\n",
      "  (4142, 1377)\t1\n",
      "  (4142, 1107)\t1\n",
      "  (4142, 813)\t1\n",
      "  (4142, 1347)\t1\n",
      "  (4142, 1195)\t1\n",
      "  (4142, 235)\t1\n",
      "  (4142, 743)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vect.fit_transform(text_x_farm_ads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiunea vocabularului: 1500\n"
     ]
    }
   ],
   "source": [
    "feature_names:list = vect.get_feature_names()\n",
    "print('Dimensiunea vocabularului:', len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'able', 'absolutely', 'abuse', 'acai', 'accept', 'access', 'accessory', 'account', 'accuracy', 'accurate', 'achieve', 'acid', 'acr', 'acreage', 'act', 'action', 'active', 'activity', 'actually']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word', 'workout', 'world', 'worth', 'wrinkle', 'write', 'written', 'wrong', 'wrote', 'ww', 'www', 'xdl', 'yahoo', 'yoga', 'yoplait', 'york', 'yourself', 'youtube', 'zealand', 'zip']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_a9e74_row0_col2,#T_a9e74_row0_col3,#T_a9e74_row0_col4,#T_a9e74_row3_col5{\n",
       "            background-color:  #ff6666;\n",
       "        }#T_a9e74_row1_col2,#T_a9e74_row1_col3,#T_a9e74_row1_col4,#T_a9e74_row1_col5{\n",
       "            background-color:  #bdfcc2;\n",
       "        }</style><table id=\"T_a9e74_\" ><thead>    <tr>        <th class=\"index_name level0\" >farm_ads_dataset</th>        <th class=\"col_heading level0 col0\" >Model_name</th>        <th class=\"col_heading level0 col1\" >Search_strategy</th>        <th class=\"col_heading level0 col2\" >train_mean_absolute_error</th>        <th class=\"col_heading level0 col3\" >train_mean_squared_error</th>        <th class=\"col_heading level0 col4\" >test_mean_absolute_error</th>        <th class=\"col_heading level0 col5\" >test_mean_squared_error</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_a9e74_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_a9e74_row0_col0\" class=\"data row0 col0\" >SGDRegressor()</td>\n",
       "                        <td id=\"T_a9e74_row0_col1\" class=\"data row0 col1\" >GridSearchCV</td>\n",
       "                        <td id=\"T_a9e74_row0_col2\" class=\"data row0 col2\" >0.453148</td>\n",
       "                        <td id=\"T_a9e74_row0_col3\" class=\"data row0 col3\" >11.951657</td>\n",
       "                        <td id=\"T_a9e74_row0_col4\" class=\"data row0 col4\" >0.437089</td>\n",
       "                        <td id=\"T_a9e74_row0_col5\" class=\"data row0 col5\" >0.382943</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_a9e74_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_a9e74_row1_col0\" class=\"data row1 col0\" >RandomForestRegressor()</td>\n",
       "                        <td id=\"T_a9e74_row1_col1\" class=\"data row1 col1\" >GridSearchCV</td>\n",
       "                        <td id=\"T_a9e74_row1_col2\" class=\"data row1 col2\" >0.074739</td>\n",
       "                        <td id=\"T_a9e74_row1_col3\" class=\"data row1 col3\" >0.017214</td>\n",
       "                        <td id=\"T_a9e74_row1_col4\" class=\"data row1 col4\" >0.163477</td>\n",
       "                        <td id=\"T_a9e74_row1_col5\" class=\"data row1 col5\" >0.079648</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_a9e74_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_a9e74_row2_col0\" class=\"data row2 col0\" >Lasso()</td>\n",
       "                        <td id=\"T_a9e74_row2_col1\" class=\"data row2 col1\" >GridSearchCV</td>\n",
       "                        <td id=\"T_a9e74_row2_col2\" class=\"data row2 col2\" >0.280764</td>\n",
       "                        <td id=\"T_a9e74_row2_col3\" class=\"data row2 col3\" >0.114929</td>\n",
       "                        <td id=\"T_a9e74_row2_col4\" class=\"data row2 col4\" >0.302685</td>\n",
       "                        <td id=\"T_a9e74_row2_col5\" class=\"data row2 col5\" >0.140455</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_a9e74_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_a9e74_row3_col0\" class=\"data row3 col0\" >MLPRegressor()</td>\n",
       "                        <td id=\"T_a9e74_row3_col1\" class=\"data row3 col1\" >GridSearchCV</td>\n",
       "                        <td id=\"T_a9e74_row3_col2\" class=\"data row3 col2\" >0.251333</td>\n",
       "                        <td id=\"T_a9e74_row3_col3\" class=\"data row3 col3\" >0.320551</td>\n",
       "                        <td id=\"T_a9e74_row3_col4\" class=\"data row3 col4\" >0.360259</td>\n",
       "                        <td id=\"T_a9e74_row3_col5\" class=\"data row3 col5\" >0.402743</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x193e4b05490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_farm_ads_data:pd.core.frame.DataFrame = dataset_regression('farm_ads_dataset', x_farm_ads, y_farm_ads)\n",
    "display(finishing(df_farm_ads_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "<div style=\"text-align: justify\">\n",
    "&emsp;&emsp;Pădurile aleatoare sau pădurile de decizie aleatoare sunt o metodă de învățare utilizată pentru clasificare, regresie și alte sarcini care operează prin construirea unei multitudini de arbori de decizie în etapa de antrenare și are ca rezultat clasa care are votul majoritar (pentru clasificare) sau media predicțiilor (pentru regresie) a arborilor individuali. Pădurile aleatoare corectează tendința arborilor de decizie de învățare excesivă a datelor de antrenare (overfit).\n",
    "    \n",
    "![random_forest_image1](./images/RandomForest.png)\n",
    "\n",
    "&emsp;&emsp;Algoritmul de antrenament pentru pădurile aleatoare aplică tehnica generală de agregare bootstrap (\"bagging\") pentru arborii de învățare. Pentru un set de antrenare cu intrările $X = x_1, ..., x_n$ și ieșirile corespunzătoare $Y = y_1, ..., y_n$ prin această tehnică aplicată de $B$ ori se selectează un subset aleator din setul de date de antrenare și antrenează câte un arbore de decizie pentru acest subset. După antrenare, predicțiile pentru exemplele nevăzute $x'$ se obțin prin calculul mediei predicțiilor a arborilor individuali pentru $x'$.<br>\n",
    "&emsp;&emsp;&emsp;&emsp;For $b = 1, ..., B$:<br>\n",
    "       &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;1. Sample, with replacement, $n$ training examples from $X$, $Y$; call these $X_b$, $Y_b$.<br>\n",
    "       &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;2. Train a classification or regression tree $f_b$ on $X_b$, $Y_b$.<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$y'=\\hat{f} = {\\frac {1}{B}}{\\sum _{b=1}^{B}f_b(x')}$<br>\n",
    "&emsp;&emsp;Folosirea metodei de agregare bootstrap conduce la performanțe mai bune deoarece reduce varianța modelului, fără a crește bias-ul. Asta înseamnă că deși predicțiile unui arbore sunt sensibile la zgomotul din subsetul său de antrenare, media predicțiilor arborilor nu este, cât timp arborii sunt independenți (antrenați pe subseturi disjuncte de date).<br>\n",
    "&emsp;&emsp;Algoritmul pădurilor aleatoare diferă de metoda descrisă prin faptul că în momentul împărțirii candidaților se selectează aleator un subset de atribute ale acestora (\"feature bagging\"), fiecare arbore având acces la un subset aleator din datele de antrenare. Astfel crește diversitatea pădurii, cea ce conduce la predicții mai robuste.<br>\n",
    "\n",
    "Printre hiper-parametrii utilizați în algoritmul pădurilor aleatoare sunt:\n",
    "- numărul de arbori, $B$ (n_estimators) pe care algoritmul îi construiește; în general un număr mai mare de arbori crește performanța și conduce la predicții mai stabile, dar scade viteza de calcul\n",
    "- numărul maxim de atribute (max_features) care să fie luate în considerare la împărțirea candidaților\n",
    "- numărul minim de frunze necesar pentru a împărți un nod intern (min_sample_leaf)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Random_forest<br>\n",
    "https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d<br>\n",
    "https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regressor\n",
    "<div style=\"text-align: justify\">\n",
    "&emsp;&emsp;Regresia \"lasou\" este o metodă analitică de regresie care efectuează atât selecția variabilelor, cât și regularizarea, pentru a spori acuratețea predicției și interpretabilitatea modelului statistic pe care îl produce. Lasso este un tip de regresie liniară care folosește \"contracția\" datelor (shrinkage). Aceata presupune reducerea datelor spre un punct central, precum media. Procedura lasso încurajează modele simple, rare (adică modele cu mai puțini parametri). Acest tip particular de regresie este bine adaptat modelelor care prezintă niveluri ridicate de muticoliniaritate sau când doriți să automatizați anumite părți ale selecției modelului, cum ar fi selectarea variabilelor / eliminarea parametrilor.\n",
    "    \n",
    "![lasso image](./images/Lasso.png)\n",
    "\n",
    "&emsp;&emsp;Acronimul \"LASSO\" reprezintă $L$east $A$bsolute $S$hrinkage and $S$election $O$perator(operator de contracție și selecție absolută minimă).<br>\n",
    "&emsp;&emsp;Regresia Lasso efectuează regularizarea L1, care adaugă o penalizare egală cu valoarea absolută a magnitudinii coeficienților. Acest tip de regularizare poate duce la modele rare, cu câțiva coeficienți; Unii dintre aceștia devenind zero și putând fi eliminați din model. Penalizările mai mari generează valori ale coeficienților mai aproape de zero, ceea ce este ideal pentru producerea de modele mai simple. Pe de altă parte, regularizarea L2 (folosită, de exemplu, în regresia Ridge) nu are ca rezultat eliminarea coeficienților sau a modelelor rare. Acest lucru face Lasso mult mai ușor de interpretat decât Ridge.<br>\n",
    "&emsp;&emsp;Soluțiile Lasso sunt probleme de programare patrate, care sunt cel mai bine rezolvate cu software, biblioteci specializate (cum ar fi scikit-learn). Scopul algoritmului este de a minimiza:\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;${\\sum _{i=1}^{n}{\\left (  y_i-{\\sum _{j}{x_{ij}\\beta_j}} \\right )^2}}+{\\alpha}{\\sum _{j=1}^{p}{|\\beta_j|}}$\n",
    "\n",
    "&emsp;&emsp;Care este același cu minimizarea sumei de pătrate cu constrângerea ${\\sum _{j=1}^{p}{|\\beta_j|}}\\leqslant s$. Unele valori de $\\beta$ sunt reduse la zero, rezultând un model de regresie mai ușor de interpretat.\n",
    "\n",
    "\n",
    "&emsp;&emsp;Printre hiper-parametrii utilizați în algoritmul Lasso din biblioteca scikit-learn este $alpha$ (puterea regularizării $L1$). $alpha$ este cu alte cuvinte proporția de reducere:\n",
    "- atunci când $alpha$ = 0, nu se elimină nici un parametru. Estimarea este egală cu cea găsită cu regresia liniară.\n",
    "- pe măsură ce $alpha$ crește, tot mai mulți coeficienți sunt setați la zero și eliminați (teoretic, atunci când $alpha$ = $\\infty$ , toți coeficienții sunt eliminați).\n",
    "- pe măsură ce $alpha$ crește, bias crește.\n",
    "- pe măsură ce $alpha$ scade, variance crește.<br>\n",
    "&emsp;&emsp;Dacă în model este inclus un intercept, acesta este de obicei lăsat neschimbat.\n",
    "\n",
    "https://www.statisticshowto.datasciencecentral.com/lasso-regression/<br>\n",
    "https://en.wikipedia.org/wiki/Lasso_(statistics)<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html<br>\n",
    "https://www.slideshare.net/kaz_yos/visual-explanation-of-ridge-regression-and-lasso<br>\n",
    "https://www.youtube.com/watch?v=NGf0voTMlcs<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron Regressor\n",
    "&emsp;&emsp;Reţelele neurale multistrat — sau perceptronii multistrat, multilayer perceptrons (MLPs) — sunt folosite pentru probleme de regresie, de clasificare şi de estimare de probabilităţi condiţionate. Instruirea este supervizată. Sunt cea mai populară variantă de reţele neurale artificiale şi fac parte din clasa mai mare a reţelelor cu propagare înainte (feed-forward). <br>\n",
    "\n",
    "&emsp;&emsp;O reţea multistrat se compune din minim trei straturi:\n",
    "- strat de intrare ce preia valorile de intrare; nu are rol computaţional, nu este format din neuroni;\n",
    "- cel puțin un strat ascuns, compus din neuroni;\n",
    "- strat de ieşire, de asemenea compus din neuroni, produce estimări de valori care sunt apoi comparate cu ieşirile dorite;\n",
    "\n",
    "![mlp image](./images/MLP.png)\n",
    "\n",
    "&emsp;&emsp;Un strat ascuns este unul care nu primeşte direct intrări şi nu produce valori de ieşire. Neuronii ascunşi produc trăsături noi pe baza vectorilor de intrare, trăsături care sunt mai apoi necesare reţelei neurale pentru producerea unei estimări. Este posibil ca o reţea să aibă mai mult de un neuron în stratul de ieşire. Se consideră că instruirea e mai eficientă dacă pe lângă valorile de intrare şi pe lângă valorile calculate de un strat de neuroni se mai furnizează o valoare constantă, de regulă +1, înmulţită cu o pondere de bias. Ponderile dintre straturi precum şi aceste ponderi de bias sunt instruibile, adică se vor modifica prin procesul de învăţare.\n",
    "\n",
    "&emsp;&emsp;Rețelele neurale cu propagare înainte precum perceptronul multistrat realizează în principal două mișcări, una înainte și una inversă. Odată ce arhitectura reţelei e fixată – numărul de straturi ascunse şi numărul de neuroni în fiecare strat precum şi funcţiile de activare – se poate trece la instruirea şi apoi utilizarea ei. Pasul de propagare înainte preia un vector de intrare $x = (x_1, . . . , x_n)^t$ şi produce modificări în starea neuronilor reţelei pornind de la intrare şi acţionând asupra succesiv straturilor $2, . . . , L −1$. Ieşirile din ultimul strat sunt\n",
    "folosite pentru predicţie – regresie, estimare de probabilitate condiţionată sau clasificare. La antrenarea rețelei, în etapa de propagare înainte, semnalul trece de la stratul de intrare, prin straturile ascunse, iar rezultatul obținut în stratul de ieșire este comparat cu cel adevărat din setul de antrenare; în etapa de propagare înapoi (backpropagation), derivatele parțiale ale funcției de eroare cu respect față de diferitele ponderi și valori de bias sunt retro-propagate prin rețea. Prin efectuarea diferenței dintre predicția rețelei și valuarea așteptată se obține un gradient față de care se actualizează parametrii rețelei, aducând perceptronul multistrat mai aproape de eroarea minimă.\n",
    "\n",
    "&emsp;&emsp;Fiecare pereche din setul de instruire $(x, d)$  va produce valoare de eroare astfel: se furnizează vectorul $x$ ca intrare în reţea şi se calculează un vector de ieşire $o$, reprezentând estimarea produsă de reţea pentru intrarea\n",
    "furnizată; se foloseşte o funcţie de cost, sau de eroare, care se doreşte a fi cu atât mai mică cu cât vectorul $o$ e mai apropiat de $d$, şi cu atât mai mare cu cât cei doi vectori sunt mai depărtaţi. În plus, se mai consideră un factor de regularizare care împiedică ponderile să devină prea mari în valoare absolută, caz asociat de regulă cu un comportament instabil al reţelei: variaţii mici ale intrării duc la salturi mari în straturile ascunse şi la ieşire.\n",
    "\n",
    "\n",
    "&emsp;&emsp; În cazul regresiei, neuronii din stratul de ieșire nu au funcție de activare (sau funcția identitate ca funcție de activare), funcția de cost fiind eroarea medie pătratică, iar ieșirea rețelei constă într-un set de valori continue. \n",
    "\n",
    "https://skymind.ai/wiki/multilayer-perceptron <br>\n",
    "https://github.com/lmsasu/cursuri/blob/master/InteligentaArtificiala/curs/InteligentaArtificiala.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDRegressor\n",
    "Regresie logistică este o modalitate de a modela un sistem discret folosind un funcția logistică (sau o variantă similară). Adică, un sistem cu ieșire are un număr finit de valori posibile. Vă puteți gândi la asta ca la un fel de clasificare algoritm (deși această descriere poate fi periculoasă, deoarece clasificarea este tehnic diferită de regresie), care mapează un set de intrări la un set finit de ieșiri.\n",
    "\n",
    "Coborârea cu gradient stochastic(SGD) este o variantă a coborâre în gradient (sau coborârea în gradient a lotului) algoritm de optimizare. În loc să utilizeze simultan toate (sau un „lot” de) date de antrenament (poate fi foarte costisitor de calcul / memorie), folosește o aproximare iterativă pentru găsirea minimului unei funcții în spațiul de intrare N-dimensional.\n",
    "\n",
    "Coborârea cu gradient stochastic poate fi folosit pentru a construi un regresie logistică model similar cu modul în care poate fi folosit pentru a construi un model de regresie liniară. Modelul în sine este independent de algoritmul de optimizare utilizat pentru antrenarea acestuia. In timp ce coborâre gradient stochastic este frecvent utilizat ca algoritm de antrenament, nu este NUMAI opțiune.\n",
    "\n",
    "\n",
    "Atat estimarea statistica cat si machine learning i-au in considerare problema minimizarii uneu functii obiectiv care are forma sumei:\n",
    "${\\displaystyle Q(w)={\\frac {1}{n}}\\sum _{i=1}^{n}Q_{i}(w),}{\\displaystyle Q(w)={\\frac {1}{n}}\\sum _{i=1}^{n}Q_{i}(w),}$\n",
    "unde parametrul  ${\\displaystyle w} {\\displaystyle Q(w)}$ Q(w) este estimat.Fiecare suma estimata ${\\displaystyle Q_{i}}Q_{i}$ este tipic asociata cu   ${\\displaystyle i}$  a i-a observatie in dataset(folosit pt antrenare).\n",
    "\n",
    "Problema sumei minimizare aprea deasemenea ca risc pentru minimizare empirica./ In acest caz, ${\\displaystyle Q_{i}(w)}Q_{i}$ (w) este valoarea functiei de loss la  ${\\displaystyle i}-lea              exemplu, si {\\displaystyle Q(w)}$ Q(w) este riscul empiric.\n",
    "\n",
    "In stochastic (or \"on-line\") gradient descent, the true gradient of {\\displaystyle Q(w)}Q(w) is approximated by a gradient at a single example:\n",
    "\n",
    "${\\displaystyle w:=w-\\eta \\nabla Q_{i}(w).}w:=w-\\eta \\nabla Q_{i}$(w).\n",
    "In timp ce algoritmul trece prin setul de antrenare,aplica updateul de mai sus pentru fiecare exemplu de antrenare.Cateva treceri pot fi facute peste setul de antrenare pana cand algoritmul converge.Daca se face asta,data poate sa fie amestecate la fiecare pas pentru a preveni ciclurire.Implementarire tipice pot folosi un ritm de invatare adaptiv pentru ca algoritmul sa convearga.\n",
    "\n",
    "![SGD image](./images/SGD.png)\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "https://ro.ec-europe.org/923532-relationship-logistic-regression-and-stochastic-AJDLNA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
